{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/RudraPramanik/applied-reinforcement-learning/blob/master/supply-chain-rl/Reinforcement_Learning_Multi_Echelon_Supply_Chain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vezVeYCHtbuL"
   },
   "source": [
    "#  A Reinforcement Learning Approach to Multi-Echelon Supply Chain Optimization\n",
    "\n",
    "This notebook presents a research-oriented prototype for **multi-echelon supply chain optimization** using **deep reinforcement learning (DRL)**. The objective is to study how learning-based control policies compare against classical inventory heuristics when operating under **uncertainty, lead times, and cost trade-offs**.\n",
    "\n",
    "Rather than assuming perfect demand knowledge or static rules, this work frames supply chain decision-making as a **sequential optimization problem**, where actions taken today affect costs and service levels in the future.\n",
    "\n",
    "---\n",
    "\n",
    "## Problem Setting\n",
    "\n",
    "We consider a **multi-echelon supply chain** consisting of:\n",
    "\n",
    "- One **central factory**\n",
    "- Multiple **downstream warehouses**\n",
    "- End customers generating **stochastic, time-varying demand**\n",
    "\n",
    "The supply chain operates over a finite planning horizon and is subject to several real-world complexities:\n",
    "\n",
    "- Uncertain customer demand (seasonal / stochastic)\n",
    "- Production and transportation **lead times**\n",
    "- Inventory holding costs at each echelon\n",
    "- Transportation costs between echelons\n",
    "- Capacity constraints\n",
    "- Penalties for unmet demand (stock-outs / backlog)\n",
    "\n",
    "The system must continuously decide:\n",
    "- How much to **produce at the factory**\n",
    "- How much to **ship to each warehouse**\n",
    "\n",
    "while balancing service level and operational cost.\n",
    "\n",
    "---\n",
    "\n",
    "##  Prototype Approach\n",
    "\n",
    "This prototype follows a structured experimental methodology:\n",
    "\n",
    "1. A **custom simulation environment** is developed to model a realistic multi-echelon supply chain.\n",
    "2. A classical **(s, Q) inventory policy** is implemented as a strong baseline.\n",
    "3. A **deep reinforcement learning agent (PPO)** is trained to control production and shipment decisions.\n",
    "4. The RL policy is evaluated and compared against the baseline under identical conditions.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "##  Scope and Limitations\n",
    "\n",
    "The reinforcement learning solution presented here is intended primarily for:\n",
    "\n",
    "- Research experimentation\n",
    "- Educational demonstration\n",
    "- Methodological comparison\n",
    "\n",
    "While RL offers modeling flexibility, it also introduces:\n",
    "- High computational cost\n",
    "- Sensitivity to reward scaling\n",
    "- Training instability without constraints\n",
    "\n",
    "##References\n",
    "- Kemmer, L., et al. Reinforcement Learning for Supply Chain Optimization.\n",
    "- Sutton & Barto, Reinforcement Learning: An Introduction\n",
    "- Schulman et al., Proximal Policy Optimization Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gz7uh8kcsEji",
    "outputId": "02afff99-7f9f-402b-e4a5-6195684e3c4c"
   },
   "outputs": [],
   "source": [
    "# Installation and Setup\n",
    "\n",
    "!pip install gymnasium -q\n",
    "!pip install stable-baselines3 -q\n",
    "!pip install shimmy -q\n",
    "!pip install matplotlib seaborn -q\n",
    "\n",
    "print(\"âœ“ All packages installed successfully!\")\n",
    "print(\"\\nPackage versions:\")\n",
    "import gymnasium\n",
    "import stable_baselines3\n",
    "print(f\"Gymnasium: {gymnasium.__version__}\")\n",
    "print(f\"Stable-Baselines3: {stable_baselines3.__version__}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XKvMVQ4jmVX0"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u15Zc0urv_d0"
   },
   "source": [
    "**Core Imports and Utility Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m6lnxQbJtHsI",
    "outputId": "5b912aa5-5eaf-4105-e45e-ee01ff4d4919"
   },
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import collections\n",
    "from typing import Tuple, List, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Stable-Baselines3 imports\n",
    "from stable_baselines3 import DQN, PPO\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Plotting style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"âœ“ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tl1B57WqwZOJ"
   },
   "source": [
    "**State and Action Space Definition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "huWlBVTftp3A",
    "outputId": "b7959018-51ee-456d-dafe-00a198edbeee"
   },
   "outputs": [],
   "source": [
    "# State and Action Classes\n",
    "\n",
    "class State:\n",
    "    \"\"\"Represents the state of the supply chain at a given time step\"\"\"\n",
    "    def __init__(self, warehouse_num: int, T: int, demand_history: List, t: int = 0):\n",
    "        self.warehouse_num = warehouse_num\n",
    "        self.factory_stock = 0\n",
    "        self.warehouse_stock = np.zeros(warehouse_num)\n",
    "        self.demand_history = demand_history\n",
    "        self.T = T\n",
    "        self.t = t\n",
    "\n",
    "    def to_array(self) -> np.ndarray:\n",
    "        \"\"\"Convert state to numpy array for neural network input\"\"\"\n",
    "        return np.concatenate([\n",
    "            [self.factory_stock],\n",
    "            self.warehouse_stock,\n",
    "            np.hstack(self.demand_history),\n",
    "            [self.t]\n",
    "        ])\n",
    "\n",
    "    def stock_levels(self) -> np.ndarray:\n",
    "        \"\"\"Get current stock levels at factory and warehouses\"\"\"\n",
    "        return np.concatenate([[self.factory_stock], self.warehouse_stock])\n",
    "\n",
    "\n",
    "class Action:\n",
    "    \"\"\"Represents production and shipping decisions\"\"\"\n",
    "    def __init__(self, warehouse_num: int):\n",
    "        self.production_level = 0\n",
    "        self.shippings_to_warehouses = np.zeros(warehouse_num)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Action(production={self.production_level:.1f}, shipments={self.shippings_to_warehouses})\"\n",
    "\n",
    "print(\"âœ“ State and Action classes defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CxvPHtFbwjds"
   },
   "source": [
    "**Multi-Echelon Supply Chain Environment (Gymnasium)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bkoyIhmvv2kG",
    "outputId": "ae44015b-4880-4781-e9a1-c8858aa7ae57"
   },
   "outputs": [],
   "source": [
    "# Cell 4: Supply Chain Environment (Gymnasium Compatible)\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "import collections\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "# Assuming State and Action classes are defined somewhere above\n",
    "\n",
    "class SupplyChainEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Custom Gymnasium Environment for Supply Chain Management\n",
    "\n",
    "    Key Modifiable Parameters:\n",
    "    - lead_times: Delivery delays for factory and warehouses\n",
    "    - demand_pattern: 'poisson', 'normal', or 'seasonal'\n",
    "    - storage_costs: Cost per unit stored\n",
    "    - transportation_costs: Cost per unit shipped\n",
    "    \"\"\"\n",
    "\n",
    "    metadata = {'render_modes': ['human']}\n",
    "\n",
    "    def __init__(self, config: Optional[dict] = None):\n",
    "        super().__init__()\n",
    "\n",
    "        # Default configuration\n",
    "        self.config = config or {}\n",
    "\n",
    "        # Episode parameters\n",
    "        self.T = self.config.get('episode_length', 52)  # 52 weeks (1 year)\n",
    "        self.warehouse_num = self.config.get('warehouse_num', 3)\n",
    "\n",
    "        # Demand parameters\n",
    "        self.demand_pattern = self.config.get('demand_pattern', 'seasonal')\n",
    "        self.d_max = self.config.get('max_demand', 10)\n",
    "        self.d_var = self.config.get('demand_variance', 3)\n",
    "\n",
    "        # Lead times\n",
    "        self.factory_lead_time = self.config.get('factory_lead_time', 2)\n",
    "        self.warehouse_lead_times = self.config.get('warehouse_lead_times', [1, 1, 2])\n",
    "\n",
    "        # Economic parameters\n",
    "        self.unit_price = self.config.get('unit_price', 100)\n",
    "        self.unit_cost = self.config.get('unit_cost', 40)\n",
    "\n",
    "        # Storage capacities\n",
    "        self.storage_capacities = np.array([30, 20, 30, 40])  # Factory, WH1, WH2, WH3\n",
    "\n",
    "        # Costs\n",
    "        self.storage_costs = self.config.get('storage_costs', np.array([2, 3, 4, 5]))\n",
    "        self.transportation_costs = self.config.get('transportation_costs', np.array([5, 7, 10]))\n",
    "        self.backlog_penalty = self.config.get('backlog_penalty', self.unit_price)\n",
    "\n",
    "        # Demand history\n",
    "        self.demand_history_len = 4\n",
    "        self.reset()\n",
    "\n",
    "        # Action space: [production_level, shipment_to_wh1, shipment_to_wh2, shipment_to_wh3]\n",
    "        self.action_space = spaces.Box(\n",
    "            low=0.0,\n",
    "            high=20.0,\n",
    "            shape=(self.warehouse_num + 1,),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "        # Observation space\n",
    "        obs_dim = (self.warehouse_num + 1) + (self.warehouse_num * self.demand_history_len) + 1\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-1000,\n",
    "            high=1000,\n",
    "            shape=(obs_dim,),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "    def _generate_demand(self, warehouse_id: int, t: int) -> float:\n",
    "        \"\"\"Generate demand based on selected pattern\"\"\"\n",
    "        if self.demand_pattern == 'seasonal':\n",
    "            base_demand = self.d_max/2 + self.d_max/2 * np.sin(\n",
    "                2 * np.pi * (t + 2 * warehouse_id) / self.T * 2\n",
    "            )\n",
    "            demand = base_demand + np.random.randint(-self.d_var, self.d_var + 1)\n",
    "        elif self.demand_pattern == 'poisson':\n",
    "            demand = np.random.poisson(lam=self.d_max/2)\n",
    "        elif self.demand_pattern == 'normal':\n",
    "            demand = np.random.normal(loc=self.d_max/2, scale=self.d_var)\n",
    "        return max(0, np.round(demand))\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        \"\"\"Reset the environment to initial state\"\"\"\n",
    "        super().reset(seed=seed)\n",
    "\n",
    "        self.demand_history = collections.deque(maxlen=self.demand_history_len)\n",
    "        for _ in range(self.demand_history_len):\n",
    "            self.demand_history.append(np.zeros(self.warehouse_num))\n",
    "\n",
    "        self.t = 0\n",
    "        self.state = State(self.warehouse_num, self.T, list(self.demand_history))\n",
    "\n",
    "        # Initialize pending orders (for lead times)\n",
    "        self.pending_production = collections.deque(maxlen=self.factory_lead_time)\n",
    "        self.pending_shipments = [\n",
    "            collections.deque(maxlen=lt) for lt in self.warehouse_lead_times\n",
    "        ]\n",
    "\n",
    "        for _ in range(self.factory_lead_time):\n",
    "            self.pending_production.append(0)\n",
    "        for i in range(self.warehouse_num):\n",
    "            for _ in range(self.warehouse_lead_times[i]):\n",
    "                self.pending_shipments[i].append(0)\n",
    "\n",
    "        return self.state.to_array(), {}\n",
    "\n",
    "    def step(self, action: np.ndarray) -> Tuple[np.ndarray, float, bool, bool, dict]:\n",
    "        \"\"\"Execute one time step\"\"\"\n",
    "        action_obj = Action(self.warehouse_num)\n",
    "        action_obj.production_level = np.clip(action[0], 0, 20)\n",
    "        action_obj.shippings_to_warehouses = np.clip(action[1:], 0, 20)\n",
    "\n",
    "        demands = np.array([\n",
    "            self._generate_demand(j, self.t) for j in range(self.warehouse_num)\n",
    "        ])\n",
    "\n",
    "        # Process lead times\n",
    "        self.pending_production.append(action_obj.production_level)\n",
    "        arriving_production = self.pending_production[0] if len(self.pending_production) > 0 else 0\n",
    "\n",
    "        arriving_shipments = np.zeros(self.warehouse_num)\n",
    "        for i in range(self.warehouse_num):\n",
    "            self.pending_shipments[i].append(action_obj.shippings_to_warehouses[i])\n",
    "            arriving_shipments[i] = self.pending_shipments[i][0] if len(self.pending_shipments[i]) > 0 else 0\n",
    "\n",
    "        # Reward calculation\n",
    "        satisfied_demand = np.minimum(self.state.warehouse_stock + arriving_shipments, demands)\n",
    "        total_revenue = self.unit_price * np.sum(satisfied_demand)\n",
    "        total_production_cost = self.unit_cost * action_obj.production_level\n",
    "        current_stocks = np.maximum(self.state.stock_levels(), 0)\n",
    "        total_storage_cost = np.dot(self.storage_costs, current_stocks)\n",
    "        stockouts = demands - satisfied_demand\n",
    "        total_backlog_penalty = self.backlog_penalty * np.sum(stockouts)\n",
    "        total_transportation_cost = np.dot(self.transportation_costs, action_obj.shippings_to_warehouses)\n",
    "        reward = (total_revenue - total_production_cost - total_storage_cost - total_backlog_penalty - total_transportation_cost)\n",
    "\n",
    "        # Update state (ðŸŸ¢ UPDATED: demand_history passed)\n",
    "        next_state = State(self.warehouse_num, self.T, list(self.demand_history), t=self.t + 1)\n",
    "        next_state.factory_stock = min(self.state.factory_stock + arriving_production - np.sum(action_obj.shippings_to_warehouses),\n",
    "                                       self.storage_capacities[0])\n",
    "        for w in range(self.warehouse_num):\n",
    "            next_state.warehouse_stock[w] = min(self.state.warehouse_stock[w] + arriving_shipments[w] - demands[w],\n",
    "                                                self.storage_capacities[w + 1])\n",
    "\n",
    "        self.demand_history.append(demands)\n",
    "        next_state.demand_history = list(self.demand_history)\n",
    "\n",
    "        self.state = next_state\n",
    "        self.t += 1\n",
    "\n",
    "        terminated = self.t >= self.T\n",
    "        truncated = False\n",
    "\n",
    "        info = {\n",
    "            'demands': demands,\n",
    "            'satisfied_demand': satisfied_demand,\n",
    "            'stockouts': stockouts,\n",
    "            'total_revenue': total_revenue,\n",
    "            'total_cost': total_production_cost + total_storage_cost + total_backlog_penalty + total_transportation_cost\n",
    "        }\n",
    "\n",
    "        return self.state.to_array(), reward, terminated, truncated, info\n",
    "\n",
    "print(\"âœ“ Supply Chain Environment created!\")\n",
    "print(\"\\nðŸ“‹ Modifiable Parameters:\")\n",
    "print(\"   - demand_pattern: 'seasonal', 'poisson', 'normal'\")\n",
    "print(\"   - factory_lead_time: Delivery delay from production\")\n",
    "print(\"   - warehouse_lead_times: Delivery delays to warehouses\")\n",
    "print(\"   - storage_costs: Cost per unit stored\")\n",
    "print(\"   - transportation_costs: Cost per unit shipped\")\n",
    "print(\"   - backlog_penalty: Penalty for stockouts\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kqpJacYuxOcr",
    "outputId": "f9afd7a2-cba5-4c40-fa03-36a1788d06a6"
   },
   "outputs": [],
   "source": [
    "# Cell 5: Test the Environment\n",
    "\n",
    "# Create environment with default settings\n",
    "env = SupplyChainEnv()\n",
    "\n",
    "# Test reset\n",
    "obs, info = env.reset()\n",
    "print(f\"âœ“ Environment reset successful\")\n",
    "print(f\"Observation shape: {obs.shape}\")\n",
    "print(f\"Action space: {env.action_space}\")\n",
    "print(f\"Observation space: {env.observation_space}\")\n",
    "\n",
    "# Test few random steps\n",
    "print(\"\\nðŸ”„ Testing random actions:\")\n",
    "for i in range(3):\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    print(f\"  Step {i+1}: Reward = {reward:.2f}, Demands = {info['demands']}\")\n",
    "\n",
    "print(\"\\nâœ“ Environment is working correctly!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pZNY1Jp7xfN3",
    "outputId": "2f9898c7-f9e0-47f6-d863-99948a7de664"
   },
   "outputs": [],
   "source": [
    "# Visualization Functions\n",
    "\n",
    "def visualize_demand_patterns(env, steps=52):\n",
    "    \"\"\"Visualize demand patterns for all warehouses\"\"\"\n",
    "    demands = np.zeros((env.warehouse_num, steps))\n",
    "\n",
    "    for t in range(steps):\n",
    "        for w in range(env.warehouse_num):\n",
    "            demands[w, t] = env._generate_demand(w, t)\n",
    "\n",
    "    plt.figure(figsize=(14, 5))\n",
    "    for w in range(env.warehouse_num):\n",
    "        plt.plot(range(steps), demands[w], marker='o', label=f'Warehouse {w+1}', alpha=0.7)\n",
    "\n",
    "    plt.xlabel('Time Step (weeks)', fontsize=12)\n",
    "    plt.ylabel('Demand (units)', fontsize=12)\n",
    "    plt.title(f'Demand Pattern: {env.demand_pattern.upper()}', fontsize=14, fontweight='bold')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def visualize_episode(env, policy=None, policy_name=\"Policy\"):\n",
    "    \"\"\"\n",
    "    Run and visualize one complete episode\n",
    "\n",
    "    Args:\n",
    "        env: Supply chain environment\n",
    "        policy: Agent/policy to test (if No, uses random actions)\n",
    "        policy_name: Name for the plot title\n",
    "    \"\"\"\n",
    "    obs, _ = env.reset()\n",
    "\n",
    "    # Storage for metrics\n",
    "    factory_stocks = []\n",
    "    warehouse_stocks = [[] for _ in range(env.warehouse_num)]\n",
    "    productions = []\n",
    "    shipments = [[] for _ in range(env.warehouse_num)]\n",
    "    rewards = []\n",
    "    demands = [[] for _ in range(env.warehouse_num)]\n",
    "\n",
    "    terminated = False\n",
    "    step = 0\n",
    "\n",
    "    while not terminated and step < env.T:\n",
    "        # Get action\n",
    "        if policy is None:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action, _ = policy.predict(obs, deterministic=True)\n",
    "\n",
    "        # Store current state\n",
    "        factory_stocks.append(env.state.factory_stock)\n",
    "        for w in range(env.warehouse_num):\n",
    "            warehouse_stocks[w].append(env.state.warehouse_stock[w])\n",
    "        productions.append(action[0])\n",
    "        for w in range(env.warehouse_num):\n",
    "            shipments[w].append(action[w + 1])\n",
    "\n",
    "        # Take step\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        rewards.append(reward)\n",
    "        for w in range(env.warehouse_num):\n",
    "            demands[w].append(info['demands'][w])\n",
    "\n",
    "        step += 1\n",
    "\n",
    "    # Create comprehensive visualization\n",
    "    fig, axes = plt.subplots(5, 1, figsize=(14, 12))\n",
    "    time_steps = range(len(rewards))\n",
    "\n",
    "    # 1. Factory Stock\n",
    "    axes[0].plot(time_steps, factory_stocks, color='purple', linewidth=2, label='Factory Stock')\n",
    "    axes[0].axhline(y=env.storage_capacities[0], color='r', linestyle='--', alpha=0.5, label='Capacity')\n",
    "    axes[0].set_ylabel('Units', fontsize=10)\n",
    "    axes[0].set_title('Factory Inventory', fontsize=11, fontweight='bold')\n",
    "    axes[0].legend(loc='upper right')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "    # 2. Warehouse Stocks\n",
    "    for w in range(env.warehouse_num):\n",
    "        axes[1].plot(time_steps, warehouse_stocks[w], linewidth=2,\n",
    "                    label=f'Warehouse {w+1}', alpha=0.7)\n",
    "    axes[1].set_ylabel('Units', fontsize=10)\n",
    "    axes[1].set_title('Warehouse Inventories', fontsize=11, fontweight='bold')\n",
    "    axes[1].legend(loc='upper right')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "    # 3. Production\n",
    "    axes[2].bar(time_steps, productions, color='blue', alpha=0.6)\n",
    "    axes[2].set_ylabel('Units', fontsize=10)\n",
    "    axes[2].set_title('Production Decisions', fontsize=11, fontweight='bold')\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "    # 4. Shipments vs Demands\n",
    "    x = np.arange(len(time_steps))\n",
    "    width = 0.2\n",
    "    for w in range(env.warehouse_num):\n",
    "        axes[3].bar(x + w*width, shipments[w], width, label=f'Ship to WH{w+1}', alpha=0.6)\n",
    "        axes[3].plot(time_steps, demands[w], marker='x', linestyle='--',\n",
    "                    label=f'Demand WH{w+1}', alpha=0.7)\n",
    "    axes[3].set_ylabel('Units', fontsize=10)\n",
    "    axes[3].set_title('Shipments vs Demands', fontsize=11, fontweight='bold')\n",
    "    axes[3].legend(loc='upper right', fontsize=8, ncol=2)\n",
    "    axes[3].grid(True, alpha=0.3)\n",
    "\n",
    "    # 5. Rewards\n",
    "    axes[4].plot(time_steps, rewards, color='green', linewidth=2, label='Step Reward')\n",
    "    axes[4].plot(time_steps, np.cumsum(rewards), color='darkgreen',\n",
    "                linewidth=2, linestyle='--', label='Cumulative Reward')\n",
    "    axes[4].set_xlabel('Time Step (weeks)', fontsize=11)\n",
    "    axes[4].set_ylabel('Reward ($)', fontsize=10)\n",
    "    axes[4].set_title('Rewards', fontsize=11, fontweight='bold')\n",
    "    axes[4].legend(loc='lower right')\n",
    "    axes[4].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.suptitle(f'{policy_name} Performance | Total Reward: ${sum(rewards):.2f}',\n",
    "                fontsize=14, fontweight='bold', y=0.995)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Print summary statistics\n",
    "    print(f\"\\nðŸ“Š Episode Summary ({policy_name}):\")\n",
    "    print(f\"   Total Reward: ${sum(rewards):.2f}\")\n",
    "    print(f\"   Average Reward per Step: ${np.mean(rewards):.2f}\")\n",
    "    print(f\"   Average Factory Stock: {np.mean(factory_stocks):.1f} units\")\n",
    "    print(f\"   Average Production: {np.mean(productions):.1f} units/week\")\n",
    "\n",
    "    return sum(rewards)\n",
    "\n",
    "print(\"âœ“ Visualization functions ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7CR-gtkJwysN"
   },
   "source": [
    "**Baseline Inventory Control Policy (s, Q)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eAOpDF1xyX3R",
    "outputId": "f40fd189-1809-4e93-f055-c9427a4e0a5a"
   },
   "outputs": [],
   "source": [
    "# Baseline (s, Q) Policy\n",
    "\n",
    "class SQPolicy:\n",
    "    \"\"\"\n",
    "    (s, Q) Reorder Policy - Baseline for comparison\n",
    "\n",
    "    Logic: When inventory falls below 's' (reorder point), order 'Q' units\n",
    "    \"\"\"\n",
    "    def __init__(self, factory_s, factory_Q, warehouse_s, warehouse_Q):\n",
    "        self.factory_s = factory_s\n",
    "        self.factory_Q = factory_Q\n",
    "        self.warehouse_s = warehouse_s  # List of reorder points\n",
    "        self.warehouse_Q = warehouse_Q  # List of reorder quantities\n",
    "\n",
    "    def predict(self, obs, deterministic=True):\n",
    "        \"\"\"Make prediction (compatible with RL interface)\"\"\"\n",
    "        # Extract state from observation\n",
    "        warehouse_num = len(self.warehouse_s)\n",
    "        factory_stock = obs[0]\n",
    "        warehouse_stocks = obs[1:warehouse_num + 1]\n",
    "\n",
    "        action = np.zeros(warehouse_num + 1)\n",
    "\n",
    "        # Reorder for warehouses if below safety stock\n",
    "        for w in range(warehouse_num):\n",
    "            if warehouse_stocks[w] < self.warehouse_s[w]:\n",
    "                action[w + 1] = self.warehouse_Q[w]\n",
    "\n",
    "        # Produce if factory stock will fall below safety level\n",
    "        total_shipments = np.sum(action[1:])\n",
    "        if factory_stock - total_shipments < self.factory_s:\n",
    "            action[0] = self.factory_Q\n",
    "\n",
    "        return action, None\n",
    "\n",
    "# baseline policy with hand-tuned parameters\n",
    "baseline_policy = SQPolicy(\n",
    "    factory_s=5,\n",
    "    factory_Q=15,\n",
    "    warehouse_s=[3, 3, 5],\n",
    "    warehouse_Q=[8, 8, 10]\n",
    ")\n",
    "\n",
    "print(\"âœ“ Baseline (s,Q) Policy created!\")\n",
    "print(f\"   Factory: Reorder {baseline_policy.factory_Q} units when stock < {baseline_policy.factory_s}\")\n",
    "print(f\"   Warehouses: Reorder points = {baseline_policy.warehouse_s}\")\n",
    "print(f\"   Warehouses: Reorder quantities = {baseline_policy.warehouse_Q}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HJs2AvY6xC-o"
   },
   "source": [
    "**Environment Rollout and Visualization Utilities**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "j0ijlipBy0sF",
    "outputId": "074576b1-c442-4f8e-f521-d59c0f6cdbdf"
   },
   "outputs": [],
   "source": [
    "#Visualize Demand Patterns and Test Baseline\n",
    "\n",
    "# Create environment with SEASONAL demand (default)\n",
    "print(\"ðŸ” Testing with SEASONAL demand pattern:\")\n",
    "env_seasonal = SupplyChainEnv(config={'demand_pattern': 'seasonal'})\n",
    "visualize_demand_patterns(env_seasonal, steps=52)\n",
    "\n",
    "# Test baseline policy\n",
    "print(\"\\nðŸŽ¯ Testing Baseline (s,Q) Policy:\")\n",
    "baseline_reward = visualize_episode(env_seasonal, baseline_policy, \"Baseline (s,Q) Policy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Vo2P47UxY5o"
   },
   "source": [
    "**Reinforcement Learning Algorithm Selection (PPO)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8rgSNwUaz0am",
    "outputId": "477cb63f-f913-4bc7-f747-aefe47e7c2c4"
   },
   "outputs": [],
   "source": [
    "# Custom Training Callback for Monitoring\n",
    "\n",
    "class TrainingCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Custom callback for monitoring training progress\n",
    "    \"\"\"\n",
    "    def __init__(self, eval_freq=1000, verbose=1):\n",
    "        super().__init__(verbose)\n",
    "        self.eval_freq = eval_freq\n",
    "        self.episode_rewards = []\n",
    "        self.episode_lengths = []\n",
    "        self.current_episode_reward = 0\n",
    "        self.current_episode_length = 0\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        # Track episode progress\n",
    "        self.current_episode_reward += self.locals['rewards']  # <- changed from [0]\n",
    "        self.current_episode_length += 1\n",
    "\n",
    "        # Check if episode ended\n",
    "        if self.locals['dones']:\n",
    "            self.episode_rewards.append(self.current_episode_reward)\n",
    "            self.episode_lengths.append(self.current_episode_length)\n",
    "\n",
    "            if self.verbose > 0 and len(self.episode_rewards) % 10 == 0:\n",
    "                avg_reward = np.mean(self.episode_rewards[-10:])\n",
    "                print(f\"Episode {len(self.episode_rewards)}: Avg Reward (last 10) = ${avg_reward:.2f}\")\n",
    "\n",
    "            self.current_episode_reward = 0\n",
    "            self.current_episode_length = 0\n",
    "\n",
    "        return True\n",
    "\n",
    "    def plot_training_progress(self):\n",
    "        \"\"\"Plot training progress\"\"\"\n",
    "        if len(self.episode_rewards) == 0:\n",
    "            print(\"No episodes completed yet!\")\n",
    "            return\n",
    "\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "        # Plot rewards\n",
    "        ax1.plot(self.episode_rewards, alpha=0.3, color='blue')\n",
    "\n",
    "        # Moving average\n",
    "        window = min(20, len(self.episode_rewards))\n",
    "        if window > 1:\n",
    "            moving_avg = np.convolve(self.episode_rewards,\n",
    "                                    np.ones(window)/window, mode='valid')\n",
    "            ax1.plot(range(window-1, len(self.episode_rewards)),\n",
    "                    moving_avg, color='red', linewidth=2, label=f'{window}-Episode Moving Avg')\n",
    "\n",
    "        ax1.set_xlabel('Episode')\n",
    "        ax1.set_ylabel('Total Reward ($)')\n",
    "        ax1.set_title('Training Progress: Episode Rewards')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "\n",
    "        # Plot episode lengths\n",
    "        ax2.plot(self.episode_lengths, alpha=0.6, color='green')\n",
    "        ax2.set_xlabel('Episode')\n",
    "        ax2.set_ylabel('Episode Length')\n",
    "        ax2.set_title('Episode Lengths')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        print(f\"\\nðŸ“ˆ Training Statistics:\")\n",
    "        print(f\"   Total Episodes: {len(self.episode_rewards)}\")\n",
    "        print(f\"   Best Reward: ${max(self.episode_rewards):.2f}\")\n",
    "        print(f\"   Average Reward (last 20): ${np.mean(self.episode_rewards[-20:]):.2f}\")\n",
    "\n",
    "print(\"âœ“ Training callback ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2GAftuZCxjlR"
   },
   "source": [
    "**PPO Agent Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "42d1532dd3b94df5bae8469600aeebc2",
      "9d934c895b944b8a9a1e238abbc8f817"
     ]
    },
    "id": "BbkFVbler4mf",
    "outputId": "4fbde55d-af80-4dfc-f9c2-40ce5b958c55"
   },
   "outputs": [],
   "source": [
    "# Train PPO Agent (Continuous Action Space)\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "print(\"ðŸ¤– Training PPO Agent...\")\n",
    "print(\"This will take a few minutes...\")\n",
    "\n",
    "# Create training environment\n",
    "env_train = SupplyChainEnv(config={\n",
    "    'demand_pattern': 'seasonal',\n",
    "    'episode_length': 52,\n",
    "    'factory_lead_time': 2,\n",
    "    'warehouse_lead_times': [1, 1, 2],\n",
    "    'storage_costs': np.array([2, 3, 4, 5]),\n",
    "    'transportation_costs': np.array([5, 7, 10]),\n",
    "    'backlog_penalty': 100\n",
    "})\n",
    "\n",
    "env_train = Monitor(env_train)\n",
    "\n",
    "# PPO model (continuous actions âœ”)\n",
    "ppo_model = PPO(\n",
    "    policy=\"MlpPolicy\",\n",
    "    env=env_train,\n",
    "    learning_rate=3e-4,\n",
    "    n_steps=2048,\n",
    "    batch_size=64,\n",
    "    n_epochs=10,\n",
    "    gamma=0.95,\n",
    "    gae_lambda=0.95,\n",
    "    clip_range=0.2,\n",
    "    ent_coef=0.01,\n",
    "    policy_kwargs=dict(\n",
    "        net_arch=dict(pi=[256, 256], vf=[256, 256])\n",
    "    ),\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Train\n",
    "ppo_model.learn(\n",
    "    total_timesteps=52 * 100,  # 100 episodes\n",
    "    progress_bar=True\n",
    ")\n",
    "\n",
    "print(\"âœ… PPO Training Complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sQTxvjoUyDax"
   },
   "source": [
    "**PPO Policy Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "X6JfBxSYzqEg",
    "outputId": "a699b1a2-1ee3-4975-f174-989936e40c7d"
   },
   "outputs": [],
   "source": [
    "# Evaluate PPO Agent\n",
    "\n",
    "print(\"ðŸŽ¯ Evaluating PPO Agent...\")\n",
    "\n",
    "env_eval = SupplyChainEnv(config={\n",
    "    'demand_pattern': 'seasonal',\n",
    "    'episode_length': 52,\n",
    "    'factory_lead_time': 2,\n",
    "    'warehouse_lead_times': [1, 1, 2],\n",
    "    'storage_costs': np.array([2, 3, 4, 5]),\n",
    "    'transportation_costs': np.array([5, 7, 10]),\n",
    "    'backlog_penalty': 100\n",
    "})\n",
    "\n",
    "#pass PPO model positionally (not as keyword)\n",
    "ppo_reward = visualize_episode(\n",
    "    env_eval,\n",
    "    ppo_model,\n",
    "    \"PPO Agent\"\n",
    ")\n",
    "\n",
    "print(f\"âœ… PPO Total Reward: ${ppo_reward:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sjTvLD2cyMaA"
   },
   "source": [
    "**Performance Comparison and Cost Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-x7775XC0JBn",
    "outputId": "e1f46b6d-cd07-411b-cb55-a45f26d1dd5d"
   },
   "outputs": [],
   "source": [
    "# Performance Comparison\n",
    "\n",
    "print(\"\\nðŸ“Š Performance Comparison:\")\n",
    "print(f\"   Baseline (s,Q) Policy: ${baseline_reward:.2f}\")\n",
    "print(f\"   PPO Agent:             ${ppo_reward:.2f}\")\n",
    "\n",
    "improvement = ppo_reward - baseline_reward\n",
    "percent_improvement = (improvement / abs(baseline_reward)) * 100\n",
    "\n",
    "print(f\"   Improvement:           ${improvement:.2f} ({percent_improvement:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tPFMEFle1Ely",
    "outputId": "226397aa-7cb6-4783-ec2c-a2f0dad645c9"
   },
   "outputs": [],
   "source": [
    "# Performance Comparison\n",
    "\n",
    "print(\"\\nðŸ† Final Performance Comparison\")\n",
    "print(f\"Baseline Policy Reward: ${baseline_reward:.2f}\")\n",
    "print(f\"PPO Agent Reward:       ${ppo_reward:.2f}\")\n",
    "\n",
    "if ppo_reward > baseline_reward:\n",
    "    print(\"âœ… PPO Agent outperforms Baseline Policy\")\n",
    "else:\n",
    "    print(\"âš  Baseline Policy outperforms PPO (consider tuning PPO)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FTHbhRIE1_nZ"
   },
   "outputs": [],
   "source": [
    "def visualize_episode(env, agent=None, title=\"\", return_rewards=False):\n",
    "    obs, _ = env.reset()\n",
    "    done = False\n",
    "\n",
    "    rewards = []\n",
    "    stocks = []\n",
    "    demands = []\n",
    "\n",
    "    while not done:\n",
    "        if agent is None:\n",
    "            # Baseline (s, Q) policy\n",
    "            action = np.array([10, 5, 5, 5], dtype=np.float32)\n",
    "        else:\n",
    "            action, _ = agent.predict(obs, deterministic=True)\n",
    "\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        rewards.append(reward)\n",
    "        stocks.append(env.state.stock_levels())\n",
    "        demands.append(info[\"demands\"])\n",
    "\n",
    "    rewards = np.array(rewards)\n",
    "\n",
    "    if return_rewards:\n",
    "        return rewards\n",
    "\n",
    "    # Visualization\n",
    "    stocks = np.array(stocks)\n",
    "    demands = np.array(demands)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    for i in range(stocks.shape[1]):\n",
    "        plt.plot(stocks[:, i], label=f\"Stock {i}\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Week\")\n",
    "    plt.ylabel(\"Inventory Level\")\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "    total_reward = rewards.sum()\n",
    "    return total_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 572
    },
    "id": "VJ6Y_dJa2IBh",
    "outputId": "b54295d9-1bfe-4503-b29e-2f75349fff75"
   },
   "outputs": [],
   "source": [
    "# Visual Comparison & Save Plot\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_comparison(env, ppo_model, save_path=\"policy_comparison.png\"):\n",
    "    baseline_rewards = visualize_episode(env, None, return_rewards=True)\n",
    "    ppo_rewards = visualize_episode(env, ppo_model, return_rewards=True)\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(baseline_rewards, label=\"Baseline Policy\")\n",
    "    plt.plot(ppo_rewards, label=\"PPO Agent\")\n",
    "\n",
    "    plt.title(\"Supply Chain Policy Comparison (Weekly Rewards)\")\n",
    "    plt.xlabel(\"Week\")\n",
    "    plt.ylabel(\"Reward\")\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300)\n",
    "\n",
    "    print(f\"ðŸ“Œ Visualization saved as {save_path}\")\n",
    "    plt.show()\n",
    "\n",
    "env_compare = SupplyChainEnv(config={\n",
    "    'demand_pattern': 'seasonal',\n",
    "    'episode_length': 52\n",
    "})\n",
    "\n",
    "visualize_comparison(env_compare, ppo_model)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMe4zjtyAmnSBxTyAJQYyPK",
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
